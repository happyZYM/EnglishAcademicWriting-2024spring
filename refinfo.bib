@inproceedings{amoozadeh-2024-TrustGenerativeAI,
  title = {Trust in {{Generative AI}} among {{Students}}: {{An}} Exploratory Study},
  shorttitle = {Trust in {{Generative AI}} among {{Students}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Amoozadeh, Matin and Daniels, David and Nam, Daye and Kumar, Aayush and Chen, Stella and Hilton, Michael and Srinivasa Ragavan, Sruti and Alipour, Mohammad Amin},
  date = {2024-03-07},
  series = {{{SIGCSE}} 2024},
  pages = {67--73},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3626252.3630842},
  url = {https://dl.acm.org/doi/10.1145/3626252.3630842},
  urldate = {2024-03-15},
  abstract = {Generative Artificial Intelligence (GenAI) systems have experienced exponential growth in the last couple of years. These systems offer exciting capabilities for CS Education (CSEd), such as generating programs, that students can well utilize for their learning. Among the many dimensions that might affect the effective adoption of GenAI for CSEd, in this paper, we investigate students' trust. Trust in GenAI influences the extent to which students adopt GenAI, in turn affecting their learning. In this paper, we present results from a survey of 253 students at two large universities to understand how much they trust GenAI tools and their feedback on how GenAI impacts their performance in CS courses. Our results show that students have different levels of trust in GenAI. We also observe different levels of confidence and motivation, highlighting the need for further understanding of factors impacting trust.},
  isbn = {9798400704239},
  keywords = {generative ai,novice programmers,trust},
  file = {C:\Users\happyZYM\Zotero\storage\NYP6XZX2\Amoozadeh 等 - 2024 - Trust in Generative AI among Students An explorat.pdf}
}

@inproceedings{benarietanay-2024-ExploratoryStudyUpperLevel,
  title = {An {{Exploratory Study}} on {{Upper-Level Computing Students}}' {{Use}} of {{Large Language Models}} as {{Tools}} in a {{Semester-Long Project}}.},
  booktitle = {Duality {{Lab}}},
  author = {{Ben Arie Tanay} and {Lexy Arinze} and {Siddhant S. Joshi} and {Kirsten A. Davis} and {James C. Davis}},
  date = {2024},
  url = {https://davisjam.github.io/publications/},
  urldate = {2024-03-15},
  eventtitle = {Annual {{Conference}} of the {{American Society}} for {{Engineering Education}}},
  langid = {english},
  file = {C\:\\Users\\happyZYM\\Zotero\\storage\\6V54AU7B\\An Exploratory Study on Upper-Level Computing Stud.pdf;C\:\\Users\\happyZYM\\Zotero\\storage\\XH9ULFMF\\publications.html}
}

@article{bernabei-2023-StudentsUseLarge,
  title = {Students' Use of Large Language Models in Engineering Education: {{A}} Case Study on Technology Acceptance, Perceptions, Efficacy, and Detection Chances},
  shorttitle = {Students' Use of Large Language Models in Engineering Education},
  author = {Bernabei, Margherita and Colabianchi, Silvia and Falegnami, Andrea and Costantino, Francesco},
  date = {2023-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {5},
  pages = {100172},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2023.100172},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000516},
  urldate = {2024-03-15},
  abstract = {The accessibility of advanced Artificial Intelligence-based tools, like ChatGPT, has made Large Language Models (LLMs) readily available to students. These LLMs can generate original written content to assist students in their academic assessments. With the rapid adoption of LLMs, exemplified by the popularity of OpenAI's ChatGPT, there is a growing need to explore their application in education. Few studies examine students' use of LLMs as learning tools. This paper focuses on the application of ChatGPT in engineering higher education through an in-depth case study. It investigates whether engineering students can generate high-quality university essays with LLMs assistance, whether existing LLMs identification systems can detect essays produced with LLMs, and how students perceive the usefulness and acceptance of LLMs in learning. The research adopts a deductive/inductive approach, combining conceptualization and empirical evidence analysis. The study involves mechanical and management engineering students, who compose essays using LLMs. The essay assessment showed good results, but some recommendations emerged for teachers and students. Thirteen LLMs detectors were tested without achieving satisfactory results, suggesting to avoid LLMs ban. In addition, students were administered a questionnaire based on constructs and items that follow the technology acceptance models available in the literature. The results contribute to qualitative evidence by highlighting possible future research and educational practices.},
  keywords = {ChatGPT,Essay generation,Higher education,LLM},
  file = {C:\Users\happyZYM\Zotero\storage\569QTEJH\S2666920X23000516.html}
}

@online{joshi-2023-GreatPowerComes,
  title = {"{{With Great Power Comes Great Responsibility}}!": {{Student}} and {{Instructor Perspectives}} on the Influence of {{LLMs}} on {{Undergraduate Engineering Education}}},
  shorttitle = {"{{With Great Power Comes Great Responsibility}}!"},
  author = {Joshi, Ishika and Budhiraja, Ritvik and Tanna, Pranav Deepak and Jain, Lovenya and Deshpande, Mihika and Srivastava, Arjun and Rallapalli, Srinivas and Akolekar, Harshal D. and Challa, Jagat Sesh and Kumar, Dhruv},
  date = {2023-09-30},
  eprint = {2309.10694},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.10694},
  url = {http://arxiv.org/abs/2309.10694},
  urldate = {2024-03-15},
  abstract = {The rise in popularity of Large Language Models (LLMs) has prompted discussions in academic circles, with students exploring LLM-based tools for coursework inquiries and instructors exploring them for teaching and research. Even though a lot of work is underway to create LLM-based tools tailored for students and instructors, there is a lack of comprehensive user studies that capture the perspectives of students and instructors regarding LLMs. This paper addresses this gap by conducting surveys and interviews within undergraduate engineering universities in India. Using 1306 survey responses among students, 112 student interviews, and 27 instructor interviews around the academic usage of ChatGPT (a popular LLM), this paper offers insights into the current usage patterns, perceived benefits, threats, and challenges, as well as recommendations for enhancing the adoption of LLMs among students and instructors. These insights are further utilized to discuss the practical implications of LLMs in undergraduate engineering education and beyond.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Emerging Technologies,Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\happyZYM\\Zotero\\storage\\3SYBYKYV\\Joshi 等 - 2023 - With Great Power Comes Great Responsibility! St.pdf;C\:\\Users\\happyZYM\\Zotero\\storage\\3IRBEPZS\\2309.html}
}

@inproceedings{joshi-2024-ChatGPTClassroomAnalysis,
  title = {{{ChatGPT}} in the {{Classroom}}: {{An Analysis}} of {{Its Strengths}} and {{Weaknesses}} for {{Solving Undergraduate Computer Science Questions}}},
  shorttitle = {{{ChatGPT}} in the {{Classroom}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Joshi, Ishika and Budhiraja, Ritvik and Dev, Harshal and Kadia, Jahnvi and Ataullah, Mohammad Osama and Mitra, Sayan and Akolekar, Harshal D. and Kumar, Dhruv},
  date = {2024-03-07},
  series = {{{SIGCSE}} 2024},
  pages = {625--631},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3626252.3630803},
  url = {https://dl.acm.org/doi/10.1145/3626252.3630803},
  urldate = {2024-03-15},
  abstract = {This research paper aims to analyze the strengths and weaknesses associated with the utilization of ChatGPT as an educational tool in the context of undergraduate computer science education. ChatGPT's usage in tasks such as solving assignments and exams has the potential to undermine students' learning outcomes and compromise academic integrity. This study adopts a quantitative approach to demonstrate the notable unreliability of ChatGPT in providing accurate answers to a wide range of questions within the field of undergraduate computer science. While the majority of existing research has concentrated on assessing the performance of Large Language Models in handling programming assignments, our study adopts a more comprehensive approach. Specifically, we evaluate various types of questions such as true/false, multi-choice, multi-select, short answer, long answer, design-based, and coding-related questions. Our evaluation highlights the potential consequences of students excessively relying on ChatGPT for the completion of assignments and exams, including self-sabotage. We conclude with a discussion on how can students and instructors constructively use ChatGPT and related tools to enhance the quality of instruction and the overall student experience.},
  isbn = {9798400704239},
  keywords = {chatgpt,computer science,education},
  file = {C:\Users\happyZYM\Zotero\storage\4ADBFJ86\Joshi 等 - 2024 - ChatGPT in the Classroom An Analysis of Its Stren.pdf}
}

@article{kosar-2024-ComputerScienceEducation,
  title = {Computer {{Science Education}} in {{ChatGPT Era}}: {{Experiences}} from an {{Experiment}} in a {{Programming Course}} for {{Novice Programmers}}},
  shorttitle = {Computer {{Science Education}} in {{ChatGPT Era}}},
  author = {Kosar, Toma\v z and Ostoji\'c, Dragana and Liu, Yu David and Mernik, Marjan},
  date = {2024-01},
  journaltitle = {Mathematics},
  volume = {12},
  number = {5},
  pages = {629},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math12050629},
  url = {https://www.mdpi.com/2227-7390/12/5/629},
  urldate = {2024-03-15},
  abstract = {The use of large language models with chatbots like ChatGPT has become increasingly popular among students, especially in Computer Science education. However, significant debates exist in the education community on the role of ChatGPT in learning. Therefore, it is critical to understand the potential impact of ChatGPT on the learning, engagement, and overall success of students in classrooms. In this empirical study, we report on a controlled experiment with 182 participants in a first-year undergraduate course on object-oriented programming. Our differential study divided students into two groups, one using ChatGPT and the other not using it for practical programming assignments. The study results showed that the students' performance is not influenced by ChatGPT usage (no statistical significance between groups with a p-value of 0.730), nor are the grading results of practical assignments (p-value 0.760) and midterm exams (p-value 0.856). Our findings from the controlled experiment suggest that it is safe for novice programmers to use ChatGPT if specific measures and adjustments are adopted in the education process.},
  issue = {5},
  langid = {english},
  keywords = {artificial intelligence,ChatGPT,controlled experiment,large language models,object-oriented programming,software engineering education},
  file = {C:\Users\happyZYM\Zotero\storage\KWKH9VFT\Kosar 等 - 2024 - Computer Science Education in ChatGPT Era Experie.pdf}
}

@inproceedings{kruger-2024-PerformanceLargeLanguage,
  title = {Performance of~{{Large Language Models}} in~a~{{Computer Science Degree Program}}},
  booktitle = {Artificial {{Intelligence}}. {{ECAI}} 2023 {{International Workshops}}},
  author = {Kr\"uger, Tim and Gref, Michael},
  editor = {Nowaczyk, S\l awomir and Biecek, Przemys\l aw and Chung, Neo Christopher and Vallati, Mauro and Skruch, Pawe\l{} and Jaworek-Korjakowska, Joanna and Parkinson, Simon and Nikitas, Alexandros and Atzm\"uller, Martin and Kliegr, Tom\'a\v s and Schmid, Ute and Bobek, Szymon and Lavrac, Nada and Peeters, Marieke and family=Dierendonck, given=Roland, prefix=van, useprefix=true and Robben, Saskia and Mercier-Laurent, Eunika and Kayakutlu, G\"ulg\"un and Owoc, Mieczyslaw Lech and Mason, Karl and Wahid, Abdul and Bruno, Pierangela and Calimeri, Francesco and Cauteruccio, Francesco and Terracina, Giorgio and Wolter, Diedrich and Leidner, Jochen L. and Kohlhase, Michael and Dimitrova, Vania},
  date = {2024},
  pages = {409--424},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-50485-3_40},
  abstract = {Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9\% of the total score in 10 tested modules, BingAI achieved 68.4\%, and LLaMa, in the 65 billion parameter variant, 20\%. Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.},
  isbn = {978-3-031-50485-3},
  langid = {english}
}

@online{liang-2023-LargeScaleSurveyUsability,
  title = {A {{Large-Scale Survey}} on the {{Usability}} of {{AI Programming Assistants}}: {{Successes}} and {{Challenges}}},
  shorttitle = {A {{Large-Scale Survey}} on the {{Usability}} of {{AI Programming Assistants}}},
  author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
  date = {2023-09-17},
  eprint = {2303.17125},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.17125},
  url = {http://arxiv.org/abs/2303.17125},
  urldate = {2024-05-02},
  abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Software Engineering},
  file = {C\:\\Users\\happyZYM\\Zotero\\storage\\LSYEDP56\\Liang 等 - 2023 - A Large-Scale Survey on the Usability of AI Progra.pdf;C\:\\Users\\happyZYM\\Zotero\\storage\\736NUMPI\\2303.html}
}

@inproceedings{liu-2024-TraditionalTeachingLarge,
  title = {Beyond {{Traditional Teaching}}: {{Large Language Models}} as {{Simulated Teaching Assistants}} in {{Computer Science}}},
  shorttitle = {Beyond {{Traditional Teaching}}},
  booktitle = {Proceedings of the 55th {{ACM Technical Symposium}} on {{Computer Science Education V}}. 1},
  author = {Liu, Mengqi and M'Hiri, Faten},
  date = {2024-03-07},
  series = {{{SIGCSE}} 2024},
  pages = {743--749},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3626252.3630789},
  url = {https://dl.acm.org/doi/10.1145/3626252.3630789},
  urldate = {2024-03-15},
  abstract = {As the prominence of Large Language Models (LLMs) grows in various sectors, their potential in education warrants exploration. In this study, we investigate the feasibility of employing GPT-3.5 from OpenAI, as an LLM teaching assistant (TA) or a virtual TA in computer science (CS) courses. The objective is to enhance the accessibility of CS education while maintaining academic integrity by refraining from providing direct solutions to current-semester assignments. Targeting Foundations of Programming (COMP202), an undergraduate course that introduces students to programming with Python, we have developed a virtual TA using the LangChain framework, known for integrating language models with diverse data sources and environments. The virtual TA assists students with their code and clarifies complex concepts. For homework questions, it is designed to guide students with hints rather than giving out direct solutions. We assessed its performance first through a qualitative evaluation, then a survey-based comparative analysis, using a mix of questions commonly asked on the COMP202 discussion board and questions created by the authors. Our preliminary results indicate that the virtual TA outperforms human TAs on clarity and engagement, matching them on accuracy when the question is non-assignment-specific, for which human TAs still proved more reliable. These findings suggest that while virtual TAs, leveraging the capabilities of LLMs, hold great promise towards making CS education experience more accessible and engaging, their optimal use necessitates human supervision. We conclude by identifying several directions that could be explored in future implementations.},
  isbn = {9798400704239},
  keywords = {adaptive teaching,chatgpt,cs education,gpt,llm,machine learning,novice programmers,openai,programming},
  file = {C:\Users\happyZYM\Zotero\storage\S9PPXQCA\Liu 和 M'Hiri - 2024 - Beyond Traditional Teaching Large Language Models.pdf}
}

@article{richards-2024-BobBotExploring,
  title = {Bob or {{Bot}}: {{Exploring ChatGPT}}'s {{Answers}} to {{University Computer Science Assessment}}},
  shorttitle = {Bob or {{Bot}}},
  author = {Richards, Mike and Waugh, Kevin and Slaymaker, Mark and Petre, Marian and Woodthorpe, John and Gooch, Daniel},
  date = {2024-01-14},
  journaltitle = {ACM Transactions on Computing Education},
  shortjournal = {ACM Trans. Comput. Educ.},
  volume = {24},
  number = {1},
  pages = {5:1--5:32},
  doi = {10.1145/3633287},
  url = {https://dl.acm.org/doi/10.1145/3633287},
  urldate = {2024-03-15},
  abstract = {Cheating has been a long-standing issue in university assessments. However, the release of ChatGPT and other free-to-use generative AI tools has provided a new and distinct method for cheating. Students can run many assessment questions through the tool and generate a superficially compelling answer, which may or may not be accurate. We ran a dual-anonymous ``quality assurance'' marking exercise across four end-of-module assessments across a distance university computer science (CS) curriculum. Each marker received five ChatGPT-generated scripts alongside 10 student scripts. A total of 90 scripts were marked; every ChatGPT-generated script for the undergraduate modules received at least a passing grade ({$>$}40\%), with all of the introductory module CS1 scripts receiving a distinction ({$>$}85\%). None of the ChatGPT-taught postgraduate scripts received a passing grade ({$>$}50\%). We also present the results of interviewing the markers and of running our sample scripts through a GPT-2 detector and the TurnItIn AI detector, which both identified every ChatGPT-generated script but differed in the number of false positives. As such, we contribute a baseline understanding of how the public release of generative AI is likely to significantly impact quality assurance processes. Our analysis demonstrates that in most cases, across a range of question formats, topics, and study levels, ChatGPT is at least capable of producing adequate answers for undergraduate assessment.},
  keywords = {ChatGPT,cheating,generative AI,quality assurance,university assessment'},
  file = {C:\Users\happyZYM\Zotero\storage\KUILT2MA\Richards 等 - 2024 - Bob or Bot Exploring ChatGPT's Answers to Univers.pdf}
}

@inproceedings{vaithilingam-2022-ExpectationVsExperience,
  title = {Expectation vs.~{{Experience}}: {{Evaluating}} the {{Usability}} of {{Code Generation Tools Powered}} by {{Large Language Models}}},
  shorttitle = {Expectation vs.~{{Experience}}},
  booktitle = {Extended {{Abstracts}} of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
  date = {2022-04-28},
  series = {{{CHI EA}} '22},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3491101.3519665},
  url = {https://dl.acm.org/doi/10.1145/3491101.3519665},
  urldate = {2024-05-02},
  abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants' feedback.},
  isbn = {978-1-4503-9156-6},
  keywords = {github copilot,large language model},
  file = {C:\Users\happyZYM\Zotero\storage\C3YUQXI9\Vaithilingam 等 - 2022 - Expectation vs. Experience Evaluating the Usabili.pdf}
}

@inproceedings{wang-2023-ExploringRoleAI,
  title = {Exploring the Role of {{AI}} Assistants in Computer Science Education: {{Methods}}, Implications, and Instructor Perspectives},
  booktitle = {2023 {{IEEE}} Symposium on Visual Languages and Human-Centric Computing ({{VL}}/{{HCC}})},
  author = {Wang, Tianjia and D\'iaz, Daniel Vargas and Brown, Chris and Chen, Yan},
  date = {2023},
  pages = {92--102},
  doi = {10.1109/VL-HCC57772.2023.00018},
  keywords = {Chatbots,ChatGPT,Computational modeling,Computer science education,Distance measurement,Education,Interview,Large language model,Shape,Visualization},
  file = {C\:\\Users\\happyZYM\\Zotero\\storage\\XLJHHJQB\\Exploring the Role of AI Assistants in Computer Sc.pdf;C\:\\Users\\happyZYM\\Zotero\\storage\\ZM2BS6TU\\10305701.html}
}
